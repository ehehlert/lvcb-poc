{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96132"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import, process, and combine the data from multiple JSON files\n",
    "\n",
    "json_files = ['../1-100.json', '../101-300.json', '../750-999.json']\n",
    "\n",
    "def process_data(data, source_file_name):\n",
    "\n",
    "    id_to_item = {item['Id']: item for item in data}\n",
    "    \n",
    "    parent_to_all_children = {}\n",
    "    for item in data:\n",
    "        if 'Relationships' in item:\n",
    "            for relationship in item['Relationships']:\n",
    "                # Initialize a dictionary for the item if it doesn't exist\n",
    "                if item['Id'] not in parent_to_all_children:\n",
    "                    parent_to_all_children[item['Id']] = {}\n",
    "                # Append the child IDs under the appropriate relationship type\n",
    "                parent_to_all_children[item['Id']].setdefault(relationship['Type'], []).extend(relationship['Ids'])\n",
    "    \n",
    "    cell_records = []\n",
    "\n",
    "    for item in data:\n",
    "        if item.get('BlockType') == 'TABLE':\n",
    "            table_id = item['Id']\n",
    "            relationships = parent_to_all_children.get(table_id, {})\n",
    "            \n",
    "            # Initialize an empty list to hold cell records, including cell_type\n",
    "            aggregated_cells = []\n",
    "            for rel_type in ['CHILD', 'MERGED_CELL', 'TABLE_FOOTER', 'TABLE_TITLE']:\n",
    "                cell_ids = relationships.get(rel_type, [])\n",
    "                for cell_id in cell_ids:\n",
    "                    # Append both cell_id and its relationship type to the list\n",
    "                    aggregated_cells.append((cell_id, rel_type))\n",
    "            \n",
    "            for cell_id, cell_type in aggregated_cells:\n",
    "                cell_block = id_to_item.get(cell_id)\n",
    "                if not cell_block:\n",
    "                    continue\n",
    "\n",
    "                entity_type = cell_block.get('EntityTypes', [None])[0] if 'EntityTypes' in cell_block else None\n",
    "\n",
    "                cell_geometry = cell_block['Geometry']['BoundingBox']\n",
    "                \n",
    "                child_ids = parent_to_all_children.get(cell_id, {}).get('CHILD', [])\n",
    "                cell_words = [id_to_item[child_id]['Text'] for child_id in child_ids if child_id in id_to_item and 'Text' in id_to_item[child_id]]\n",
    "                cell_content = ' '.join(cell_words)\n",
    "                \n",
    "                cell_records.append({\n",
    "                    'cell_id': cell_id,\n",
    "                    'cell_type': cell_type,  # Include the cell_type here\n",
    "                    'entity_type': entity_type,\n",
    "                    'cell_words': cell_words,\n",
    "                    'cell_content': cell_content,\n",
    "                    'cell_width': cell_geometry['Width'],\n",
    "                    'cell_height': cell_geometry['Height'],\n",
    "                    'cell_left': cell_geometry['Left'],\n",
    "                    'cell_top': cell_geometry['Top'],\n",
    "                    'row_index': cell_block.get('RowIndex', None),\n",
    "                    'column_index': cell_block.get('ColumnIndex', None),\n",
    "                    'row_span': cell_block.get('RowSpan', 1),\n",
    "                    'column_span': cell_block.get('ColumnSpan', 1),\n",
    "                    'table_id': table_id,\n",
    "                    'table_type': item['EntityTypes'][0] if 'EntityTypes' in item else None,\n",
    "                    'table_width': item['Geometry']['BoundingBox']['Width'],\n",
    "                    'table_height': item['Geometry']['BoundingBox']['Height'],\n",
    "                    'table_left': item['Geometry']['BoundingBox']['Left'],\n",
    "                    'table_top': item['Geometry']['BoundingBox']['Top'],\n",
    "                    'table_page': item['Page'],\n",
    "                    'source': source_file_name,\n",
    "                    # Additional fields can be added here\n",
    "                })\n",
    "    return pd.DataFrame(cell_records)\n",
    "\n",
    "# Process each file and combine the results as before\n",
    "cells_dfs = [process_data(data, file_path.split('/')[-1]) for file_path in json_files for data in [json.load(open(file_path))]]\n",
    "cells_dfs = pd.concat(cells_dfs, ignore_index=True)\n",
    "\n",
    "# Display the length of the dataframe\n",
    "len(cells_dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96132"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Establish meaningful cell content for each cell by examining merged cell relationships\n",
    "\n",
    "\n",
    "# Ensure the necessary columns are in the correct data type\n",
    "cells_dfs['row_index'] = cells_dfs['row_index'].fillna(0).astype(int)\n",
    "cells_dfs['column_index'] = cells_dfs['column_index'].fillna(0).astype(int)\n",
    "cells_dfs['row_span'] = cells_dfs['row_span'].fillna(1).astype(int)  # Default span of 1 if missing\n",
    "cells_dfs['column_span'] = cells_dfs['column_span'].fillna(1).astype(int)\n",
    "\n",
    "# Sort the DataFrame as required\n",
    "cells_dfs.sort_values(by=['table_id', 'column_index', 'row_index'], inplace=True)\n",
    "\n",
    "# Isolate merged cells\n",
    "merged_cells = cells_dfs[cells_dfs['cell_type'] == 'MERGED_CELL']\n",
    "\n",
    "# Initialize a column for tracking merged cell parent ID and merge status\n",
    "cells_dfs['merged_parent_cell_id'] = np.nan\n",
    "cells_dfs['has_merged_parent'] = 0\n",
    "\n",
    "for cell in merged_cells.itertuples():\n",
    "    # Calculate the affected range of rows and columns\n",
    "    affected_rows = range(cell.row_index, cell.row_index + cell.row_span)\n",
    "    affected_columns = range(cell.column_index, cell.column_index + cell.column_span)\n",
    "\n",
    "    # Find the cells that are affected\n",
    "    affected_cells = cells_dfs[\n",
    "        (cells_dfs['table_id'] == cell.table_id) &\n",
    "        (cells_dfs['cell_type'] == 'CHILD') &  # Targeting only child cells\n",
    "        (cells_dfs['row_index'].isin(affected_rows)) &\n",
    "        (cells_dfs['column_index'].isin(affected_columns))\n",
    "    ]\n",
    "\n",
    "    # Aggregate text content of affected cells, stripping to remove leading/trailing spaces\n",
    "    aggregated_text_content = \" \".join(filter(None, affected_cells['cell_content'].astype(str))).strip()\n",
    "\n",
    "    if aggregated_text_content:\n",
    "        # Update the affected cells with the aggregated text content and merge-related information\n",
    "        cells_dfs.loc[affected_cells.index, 'cell_content'] = aggregated_text_content\n",
    "        cells_dfs.loc[affected_cells.index, 'merged_parent_cell_id'] = cell.cell_id\n",
    "        cells_dfs.loc[affected_cells.index, 'has_merged_parent'] = 1\n",
    "\n",
    "# Fill missing values for new columns\n",
    "cells_dfs['has_merged_parent'] = cells_dfs['has_merged_parent'].fillna(0).astype(int)\n",
    "# Do not convert merged_parent_cell_id to int; leave it as is or ensure it's treated as a string/object\n",
    "cells_dfs['merged_parent_cell_id'] = cells_dfs['merged_parent_cell_id'].fillna('None')\n",
    "\n",
    "# Optional: If you want to ensure 'merged_parent_cell_id' is explicitly recognized as a string/object column:\n",
    "cells_dfs['merged_parent_cell_id'] = cells_dfs['merged_parent_cell_id'].astype(str)\n",
    "\n",
    "# Display the length of the DataFrame\n",
    "len(cells_dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2703\n"
     ]
    }
   ],
   "source": [
    "# Summarize cell contents and child entities for each table in tables_df_without_titles\n",
    "\n",
    "\n",
    "# Function to aggregate cell contents into a list of lists, one per row\n",
    "def aggregate_contents(group):\n",
    "    # Sort the group by row and column index to ensure the correct order\n",
    "    sorted_group = group.sort_values(by=['row_index', 'column_index'])\n",
    "    # Aggregate contents by row\n",
    "    contents_by_row = sorted_group.groupby('row_index')['cell_words'].apply(list).tolist()\n",
    "    return contents_by_row\n",
    "\n",
    "def aggregate_child_entities(group):\n",
    "    # Filter the group to only include CHILD cells\n",
    "    child_cells = group[group['cell_type'] == 'CHILD']\n",
    "    # Replace NaN or empty entity_type values with 'normal'\n",
    "    child_cells['entity_type'] = child_cells['entity_type'].replace({np.nan: 'normal', '': 'normal'})\n",
    "    # Sort the group by row and column index to ensure the correct order\n",
    "    sorted_group = child_cells.sort_values(by=['row_index', 'column_index'])\n",
    "    # Aggregate entity types by row\n",
    "    entities_by_row = sorted_group.groupby('row_index')['entity_type'].apply(list).tolist()\n",
    "    return entities_by_row\n",
    "\n",
    "# Aggregate information for each table\n",
    "tables_df_without_titles = cells_dfs.groupby('table_id').apply(lambda g: pd.Series({\n",
    "    'table_width': g['table_width'].max(),\n",
    "    'table_height': g['table_height'].max(),\n",
    "    'table_left': g['table_left'].max(),\n",
    "    'table_top': g['table_top'].max(),\n",
    "    'table_page': g['table_page'].max(),\n",
    "    'source': g['source'].iloc[0],\n",
    "    'cell_count': g['cell_words'].count(),\n",
    "    'row_count': int(g['row_index'].max()),\n",
    "    'column_count': int(g['column_index'].max()),\n",
    "    'content': aggregate_contents(g),\n",
    "    'entities': aggregate_child_entities(g),\n",
    "    # Add counts for different cell types\n",
    "    'child_count': g[g['cell_type'] == 'CHILD']['cell_type'].count(),\n",
    "    'merged_cell_count': g[g['cell_type'] == 'MERGED_CELL']['cell_type'].count(),\n",
    "    'table_title_count': g[g['cell_type'] == 'TABLE_TITLE']['cell_type'].count(),\n",
    "    'table_footer_count': g[g['cell_type'] == 'TABLE_FOOTER']['cell_type'].count(),\n",
    "    'table_type': g['table_type'].max()\n",
    "})).reset_index()\n",
    "\n",
    "\n",
    "# Calculate page title per page of each document based on confidence scores, store in titles_df, and merge with tables_df_without_titles as tables_df\n",
    "\n",
    "\n",
    "def process_layout_titles(data, source_file_name):\n",
    "    layout_title_ids = [item['Id'] for item in data if item.get('BlockType') == 'LAYOUT_TITLE']\n",
    "    id_to_item = {item['Id']: item for item in data}\n",
    "    \n",
    "    layout_titles = []\n",
    "    for layout_title in layout_title_ids:\n",
    "        layout_title_block = id_to_item[layout_title]\n",
    "        layout_title_cell = {\n",
    "            'layout_title_id': layout_title,\n",
    "            'layout_title_text': ' '.join([id_to_item[child_id]['Text'] for child_id in layout_title_block.get('Relationships', [{}])[0].get('Ids', []) if child_id in id_to_item and 'Text' in id_to_item[child_id]]),\n",
    "            'layout_title_page': layout_title_block['Page'],\n",
    "            'layout_title_confidence': layout_title_block['Confidence'],\n",
    "            'source': source_file_name,  # Keep track of the source document\n",
    "        }\n",
    "        layout_titles.append(layout_title_cell)\n",
    "\n",
    "    doc_titles_df = pd.DataFrame(layout_titles)\n",
    "    \n",
    "    # Perform calculations within the current document's scope\n",
    "    doc_titles_df['max_confidence_per_page'] = doc_titles_df.groupby('layout_title_page')['layout_title_confidence'].transform('max')\n",
    "    doc_titles_df['is_max_confidence'] = doc_titles_df['layout_title_confidence'] == doc_titles_df['max_confidence_per_page']\n",
    "    doc_titles_df.drop(columns=['max_confidence_per_page'], inplace=True)\n",
    "    \n",
    "    return doc_titles_df\n",
    "\n",
    "# Initialize an empty list to hold DataFrames from all files\n",
    "titles_df = []\n",
    "\n",
    "# Process each JSON file separately and append the results to the list\n",
    "for file_path in json_files:\n",
    "    with open(file_path) as file:\n",
    "        data = json.load(file)\n",
    "        doc_titles_df = process_layout_titles(data, file_path.split('/')[-1])\n",
    "        titles_df.append(doc_titles_df)\n",
    "\n",
    "# Concatenate all DataFrames after processing\n",
    "titles_df = pd.concat(titles_df, ignore_index=True)\n",
    "\n",
    "tables_df = pd.merge(tables_df_without_titles, titles_df[titles_df['is_max_confidence'] == True][['source', 'layout_title_page', 'layout_title_text']],\n",
    "                     left_on=['source', 'table_page'], right_on=['source', 'layout_title_page'], how='left')\n",
    "\n",
    "tables_df.drop(columns=['layout_title_page'], inplace=True, errors='ignore')\n",
    "\n",
    "print(len(tables_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed one-hot encoding for table_type: 2703\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing steps for the table content and entities\n",
    "\n",
    "\n",
    "## ONE-HOT ENCODING FOR TABLE_TYPE\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "\n",
    "table_type_encoded = one_hot_encoder.fit_transform(tables_df[['table_type']])\n",
    "table_type_encoded_dense = table_type_encoded.toarray()\n",
    "column_names = one_hot_encoder.get_feature_names_out(['table_type'])\n",
    "table_type_encoded_df = pd.DataFrame(table_type_encoded_dense, columns=column_names)\n",
    "\n",
    "tables_df = pd.concat([tables_df.reset_index(drop=True), table_type_encoded_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "print('Completed one-hot encoding for table_type:', len(tables_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "\n",
    "tfidf_vectorizer_content = load('../corpuses//tfidf_vectorizer_content.joblib')\n",
    "tfidf_vectorizer_titles = load('../corpuses/tfidf_vectorizer_titles.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_content_corpus = tables_df['content'].apply(lambda x: ' '.join(map(str, x)))\n",
    "new_titles_corpus = tables_df['layout_title_text'].apply(lambda x: str(x))  # Assuming this is already a string or similar operation if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_layout_title_features = tfidf_vectorizer_titles.transform(new_titles_corpus)\n",
    "tfidf_content_features = tfidf_vectorizer_content.transform(new_content_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For content features\n",
    "content_feature_names = [f'content_tfidf_{i}' for i in range(tfidf_content_features.shape[1])]\n",
    "tfidf_content_df = pd.DataFrame(tfidf_content_features.toarray(), columns=content_feature_names)\n",
    "\n",
    "# For title features\n",
    "title_feature_names = [f'title_tfidf_{i}' for i in range(tfidf_layout_title_features.shape[1])]\n",
    "tfidf_layout_title_df = pd.DataFrame(tfidf_layout_title_features.toarray(), columns=title_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index on the original DataFrame if necessary to ensure alignment\n",
    "tables_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Concatenate the original DataFrame with the new TF-IDF DataFrames\n",
    "tables_df = pd.concat([tables_df, tfidf_content_df, tfidf_layout_title_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique entity types: {'normal', 'TABLE_SECTION_TITLE', 'COLUMN_HEADER', 'TABLE_FOOTER', 'TABLE_TITLE'}\n",
      "Entity type to integer mapping: {'COLUMN_HEADER': 1, 'TABLE_FOOTER': 2, 'TABLE_SECTION_TITLE': 3, 'TABLE_TITLE': 4, 'normal': 5}\n",
      "\n",
      "Max table rows observed in training dataset: 16\n",
      "Max table columns observed in training dataset: 6\n",
      "\n",
      "Padded all entities lists to contain 16 rows and 6 columns\n",
      "\n",
      "New shape of entities feature: (2703, 96)\n"
     ]
    }
   ],
   "source": [
    "## LABEL ENCODING FOR ENTITIES\n",
    "def pad_all_lists(list_of_list_of_lists, pad_value=0):\n",
    "    # Determine the longest inner list length\n",
    "    max_inner_length = max(len(inner) for outer in list_of_list_of_lists for inner in outer)\n",
    "    \n",
    "    # Determine the max number of inner lists within any outer list\n",
    "    max_outer_length = max(len(outer) for outer in list_of_list_of_lists)\n",
    "    \n",
    "    # Pad inner lists\n",
    "    padded_inner = [[inner + [pad_value] * (max_inner_length - len(inner)) for inner in outer] for outer in list_of_list_of_lists]\n",
    "    \n",
    "    # Pad outer lists to ensure they all have the same number of inner lists, with each inner list padded to the same length\n",
    "    padded_outer = [outer + [[pad_value] * max_inner_length] * (max_outer_length - len(outer)) for outer in padded_inner]\n",
    "    \n",
    "    return padded_outer\n",
    "\n",
    "\n",
    "# Extract all unique entity types\n",
    "unique_entity_types = set(entity for sublist in tables_df['entities'] for item in sublist for entity in item)\n",
    "\n",
    "# Map unique entity types to integers starting from 1\n",
    "entity_type_to_int = {entity: i + 1 for i, entity in enumerate(sorted(unique_entity_types))}\n",
    "\n",
    "# Transform the entities lists using the mapping\n",
    "tables_df['entities_int'] = tables_df['entities'].apply(lambda x: [[entity_type_to_int[entity] for entity in sublist] for sublist in x])\n",
    "\n",
    "print(\"\\nUnique entity types:\", unique_entity_types)\n",
    "print(\"Entity type to integer mapping:\", entity_type_to_int)\n",
    "\n",
    "# Apply padding to the 'entities_int' column\n",
    "tables_df['entities_int_padded'] = pad_all_lists(tables_df['entities_int'])\n",
    "tables_df.drop('entities_int', axis=1, inplace=True) \n",
    "\n",
    "print(f'\\nMax table rows observed in training dataset: {tables_df[\"row_count\"].max()}')\n",
    "print(f'Max table columns observed in training dataset: {tables_df[\"column_count\"].max()}')\n",
    "print(f'\\nPadded all entities lists to contain {len(tables_df[\"entities_int_padded\"][0])} rows and {len(tables_df[\"entities_int_padded\"][0][0])} columns')\n",
    "\n",
    "## Flatten the entities_int_padded column\n",
    "flattened_entities = np.array([np.array(row).flatten() for row in tables_df['entities_int_padded']])\n",
    "\n",
    "print(f\"\\nNew shape of entities feature: {flattened_entities.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_df.to_pickle('tables_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_df.to_csv('tables_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following are for further processing cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_id</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>cell_words</th>\n",
       "      <th>cell_content</th>\n",
       "      <th>cell_width</th>\n",
       "      <th>cell_height</th>\n",
       "      <th>cell_left</th>\n",
       "      <th>cell_top</th>\n",
       "      <th>row_index</th>\n",
       "      <th>column_index</th>\n",
       "      <th>row_span</th>\n",
       "      <th>column_span</th>\n",
       "      <th>table_id</th>\n",
       "      <th>source</th>\n",
       "      <th>merged_parent_cell_id</th>\n",
       "      <th>has_merged_parent</th>\n",
       "      <th>words_above</th>\n",
       "      <th>words_left</th>\n",
       "      <th>words_below</th>\n",
       "      <th>words_right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>428073c6-299d-4272-bbd3-d0ac3bfa59b6</td>\n",
       "      <td>TABLE_TITLE</td>\n",
       "      <td>[Trip, Unit, Settings]</td>\n",
       "      <td>Trip Unit Settings</td>\n",
       "      <td>0.845572</td>\n",
       "      <td>0.021403</td>\n",
       "      <td>0.086389</td>\n",
       "      <td>0.463036</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0003dc62-ff11-4a22-b5c4-e2c9a9de8137</td>\n",
       "      <td>750-999.json</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>d8f19ff7-799f-4a91-af81-e9fb623cd012</td>\n",
       "      <td>CHILD</td>\n",
       "      <td>[Trip, Unit, Settings]</td>\n",
       "      <td>Trip Unit Settings</td>\n",
       "      <td>0.221201</td>\n",
       "      <td>0.021520</td>\n",
       "      <td>0.087440</td>\n",
       "      <td>0.462512</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0003dc62-ff11-4a22-b5c4-e2c9a9de8137</td>\n",
       "      <td>750-999.json</td>\n",
       "      <td>3794ae51-54bd-43ca-90ab-79c08b479a2d</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Element, Long Time Element Settings, Long Tim...</td>\n",
       "      <td>[empty, empty, empty, empty]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>3794ae51-54bd-43ca-90ab-79c08b479a2d</td>\n",
       "      <td>MERGED_CELL</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>0.845572</td>\n",
       "      <td>0.021962</td>\n",
       "      <td>0.087640</td>\n",
       "      <td>0.462069</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0003dc62-ff11-4a22-b5c4-e2c9a9de8137</td>\n",
       "      <td>750-999.json</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>db6d13c1-2282-4577-91c6-eeb11fdda4c1</td>\n",
       "      <td>CHILD</td>\n",
       "      <td>[Element]</td>\n",
       "      <td>Element</td>\n",
       "      <td>0.221002</td>\n",
       "      <td>0.019851</td>\n",
       "      <td>0.087650</td>\n",
       "      <td>0.483867</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0003dc62-ff11-4a22-b5c4-e2c9a9de8137</td>\n",
       "      <td>750-999.json</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>[Trip Unit Settings]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Long Time Element Settings, Long Time Pick Up...</td>\n",
       "      <td>[Ranges, As Found, As Left, As Tested]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>a441f1f5-2720-4118-9dc8-95493f9d62b8</td>\n",
       "      <td>CHILD</td>\n",
       "      <td>[Long, Time, Element, Settings]</td>\n",
       "      <td>Long Time Element Settings</td>\n",
       "      <td>0.221202</td>\n",
       "      <td>0.019851</td>\n",
       "      <td>0.087461</td>\n",
       "      <td>0.503555</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0003dc62-ff11-4a22-b5c4-e2c9a9de8137</td>\n",
       "      <td>750-999.json</td>\n",
       "      <td>b36b94e0-1c4d-48e3-8021-d2bd1975a088</td>\n",
       "      <td>1</td>\n",
       "      <td>[Trip Unit Settings, Element]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Long Time Pick Up:, Long Time Delay:, Short T...</td>\n",
       "      <td>[empty, empty, empty, empty]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  cell_id    cell_type  \\\n",
       "NaN  428073c6-299d-4272-bbd3-d0ac3bfa59b6  TABLE_TITLE   \n",
       "0.0  d8f19ff7-799f-4a91-af81-e9fb623cd012        CHILD   \n",
       "NaN  3794ae51-54bd-43ca-90ab-79c08b479a2d  MERGED_CELL   \n",
       "1.0  db6d13c1-2282-4577-91c6-eeb11fdda4c1        CHILD   \n",
       "2.0  a441f1f5-2720-4118-9dc8-95493f9d62b8        CHILD   \n",
       "\n",
       "                          cell_words                cell_content  cell_width  \\\n",
       "NaN           [Trip, Unit, Settings]          Trip Unit Settings    0.845572   \n",
       "0.0           [Trip, Unit, Settings]          Trip Unit Settings    0.221201   \n",
       "NaN                               []                                0.845572   \n",
       "1.0                        [Element]                     Element    0.221002   \n",
       "2.0  [Long, Time, Element, Settings]  Long Time Element Settings    0.221202   \n",
       "\n",
       "     cell_height  cell_left  cell_top  row_index  column_index  row_span  \\\n",
       "NaN     0.021403   0.086389  0.463036          0             0         1   \n",
       "0.0     0.021520   0.087440  0.462512          1             1         1   \n",
       "NaN     0.021962   0.087640  0.462069          1             1         1   \n",
       "1.0     0.019851   0.087650  0.483867          2             1         1   \n",
       "2.0     0.019851   0.087461  0.503555          3             1         1   \n",
       "\n",
       "     column_span                              table_id        source  \\\n",
       "NaN            1  0003dc62-ff11-4a22-b5c4-e2c9a9de8137  750-999.json   \n",
       "0.0            1  0003dc62-ff11-4a22-b5c4-e2c9a9de8137  750-999.json   \n",
       "NaN            5  0003dc62-ff11-4a22-b5c4-e2c9a9de8137  750-999.json   \n",
       "1.0            1  0003dc62-ff11-4a22-b5c4-e2c9a9de8137  750-999.json   \n",
       "2.0            1  0003dc62-ff11-4a22-b5c4-e2c9a9de8137  750-999.json   \n",
       "\n",
       "                    merged_parent_cell_id  has_merged_parent  \\\n",
       "NaN                                  None                  0   \n",
       "0.0  3794ae51-54bd-43ca-90ab-79c08b479a2d                  1   \n",
       "NaN                                  None                  0   \n",
       "1.0                                  None                  0   \n",
       "2.0  b36b94e0-1c4d-48e3-8021-d2bd1975a088                  1   \n",
       "\n",
       "                       words_above words_left  \\\n",
       "NaN                            NaN        NaN   \n",
       "0.0                             []         []   \n",
       "NaN                            NaN        NaN   \n",
       "1.0           [Trip Unit Settings]         []   \n",
       "2.0  [Trip Unit Settings, Element]         []   \n",
       "\n",
       "                                           words_below  \\\n",
       "NaN                                                NaN   \n",
       "0.0  [Element, Long Time Element Settings, Long Tim...   \n",
       "NaN                                                NaN   \n",
       "1.0  [Long Time Element Settings, Long Time Pick Up...   \n",
       "2.0  [Long Time Pick Up:, Long Time Delay:, Short T...   \n",
       "\n",
       "                                words_right  \n",
       "NaN                                     NaN  \n",
       "0.0            [empty, empty, empty, empty]  \n",
       "NaN                                     NaN  \n",
       "1.0  [Ranges, As Found, As Left, As Tested]  \n",
       "2.0            [empty, empty, empty, empty]  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## NOTE: THIS TAKES AROUND 27.5 MINUTES TO RUN FOR 100,000 ROWS\n",
    "\n",
    "# Sort the DataFrame as required\n",
    "combined_cells_df.sort_values(by=['table_id', 'column_index', 'row_index'], inplace=True)\n",
    "\n",
    "# Initialize empty lists to store the 'words' in each direction for each cell\n",
    "words_above_list = []\n",
    "words_left_list = []\n",
    "words_below_list = []\n",
    "words_right_list = []\n",
    "\n",
    "# Filter the DataFrame to only include rows where cell_type is 'CHILD'\n",
    "child_cells_df = combined_cells_df[combined_cells_df['cell_type'] == 'CHILD']\n",
    "\n",
    "# Iterate over rows of child_cells_df instead of the entire cells_df\n",
    "for index, row in child_cells_df.iterrows():\n",
    "    # Adjust masks to include only CHILD cells for comparison\n",
    "    above_mask = (combined_cells_df['table_id'] == row['table_id']) & \\\n",
    "                 (combined_cells_df['column_index'] == row['column_index']) & \\\n",
    "                 (combined_cells_df['row_index'] < row['row_index']) & \\\n",
    "                 (combined_cells_df['cell_type'] == 'CHILD')\n",
    "    above_cells = combined_cells_df.loc[above_mask]\n",
    "\n",
    "    left_mask = (combined_cells_df['table_id'] == row['table_id']) & \\\n",
    "                (combined_cells_df['row_index'] == row['row_index']) & \\\n",
    "                (combined_cells_df['column_index'] < row['column_index']) & \\\n",
    "                (combined_cells_df['cell_type'] == 'CHILD')\n",
    "    left_cells = combined_cells_df.loc[left_mask]\n",
    "\n",
    "    below_mask = (combined_cells_df['table_id'] == row['table_id']) & \\\n",
    "                 (combined_cells_df['column_index'] == row['column_index']) & \\\n",
    "                 (combined_cells_df['row_index'] > row['row_index']) & \\\n",
    "                 (combined_cells_df['cell_type'] == 'CHILD')\n",
    "    below_cells = combined_cells_df.loc[below_mask]\n",
    "\n",
    "    right_mask = (combined_cells_df['table_id'] == row['table_id']) & \\\n",
    "                 (combined_cells_df['row_index'] == row['row_index']) & \\\n",
    "                 (combined_cells_df['column_index'] > row['column_index']) & \\\n",
    "                 (combined_cells_df['cell_type'] == 'CHILD')\n",
    "    right_cells = combined_cells_df.loc[right_mask]\n",
    "    \n",
    "    # Process each direction's cells to aggregate words, replacing None with 'empty' and joining words within a cell\n",
    "    words_above = [' '.join([word if word is not None else 'empty' for word in cell_words]) if cell_words else 'empty' for cell_words in above_cells['cell_words']]\n",
    "    words_left = [' '.join([word if word is not None else 'empty' for word in cell_words]) if cell_words else 'empty' for cell_words in left_cells['cell_words']]\n",
    "    words_below = [' '.join([word if word is not None else 'empty' for word in cell_words]) if cell_words else 'empty' for cell_words in below_cells['cell_words']]\n",
    "    words_right = [' '.join([word if word is not None else 'empty' for word in cell_words]) if cell_words else 'empty' for cell_words in right_cells['cell_words']]\n",
    "\n",
    "    # Append the list to the respective direction list\n",
    "    words_above_list.append(words_above)\n",
    "    words_left_list.append(words_left)\n",
    "    words_below_list.append(words_below)\n",
    "    words_right_list.append(words_right)\n",
    "\n",
    "# Since we're iterating over child_cells_df, we need to merge the results back into the original DataFrame\n",
    "# Create a temporary DataFrame with the results\n",
    "temp_df = pd.DataFrame({\n",
    "    'index': child_cells_df.index,\n",
    "    'words_above': words_above_list,\n",
    "    'words_left': words_left_list,\n",
    "    'words_below': words_below_list,\n",
    "    'words_right': words_right_list\n",
    "})\n",
    "\n",
    "# Merge the temporary DataFrame back into the original DataFrame based on the index\n",
    "combined_cells_df = pd.merge(combined_cells_df, temp_df, how='left', left_index=True, right_on='index')\n",
    "\n",
    "# Drop the 'index' column as it's no longer needed\n",
    "combined_cells_df.drop(columns=['index'], inplace=True)\n",
    "\n",
    "combined_cells_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_cells_df.to_csv('combined_cells3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is probably not applicable anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New shape of content_string feature: (2703, 1000)\n",
      "New shape of layout_title_text feature: (2703, 16)\n"
     ]
    }
   ],
   "source": [
    "## TF-IDF VECTORIZATION FOR MEANINGFUL STRINGS\n",
    "\n",
    "# Create content_string column from content column\n",
    "tables_df['content_string'] = tables_df['content'].apply(lambda x: ' '.join(map(str, x)))\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "\n",
    "# Fit and transform the text fields to TF-IDF weighted matrices\n",
    "tfidf_content = tfidf_vectorizer.fit_transform(tables_df['content_string'])\n",
    "tfidf_layout_title_text = tfidf_vectorizer.fit_transform(tables_df['layout_title_text'])\n",
    "\n",
    "# Convert the TF-IDF matricies to dense arrays\n",
    "tfidf_content = tfidf_content.toarray()\n",
    "tfidf_layout_title_text = tfidf_layout_title_text.toarray()\n",
    "\n",
    "tfidf_content_df = pd.DataFrame(tfidf_content, columns=[f'content_tfidf_{i+1}' for i in range(tfidf_content.shape[1])])\n",
    "tfidf_layout_title_text_df = pd.DataFrame(tfidf_layout_title_text, columns=[f'layout_title_tfidf_{i+1}' for i in range(tfidf_layout_title_text.shape[1])])\n",
    "tables_df.reset_index(drop=True, inplace=True)\n",
    "tables_df = pd.concat([tables_df, tfidf_content_df, tfidf_layout_title_text_df], axis=1)\n",
    "\n",
    "# Check the shape of the resulting TF-IDF matrix\n",
    "print(f\"\\nNew shape of content_string feature: {tfidf_content.shape}\")\n",
    "print(f\"New shape of layout_title_text feature: {tfidf_layout_title_text.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
